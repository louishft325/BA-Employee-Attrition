---
title: "Attrition Data Analysis"
output:
  pdf_document: default
  html_document:
    keep_md: true
---

This is our Analysis Notebook.

```{r}

# Install and load necessary libraries
if (!requireNamespace("caret", quietly = TRUE)) {
  install.packages("caret")
}
if (!requireNamespace("skimr", quietly = TRUE)) {
  install.packages("skimr")
}
if (!requireNamespace("tidyverse", quietly = TRUE)) {
  install.packages("tidyverse")
}
if (!requireNamespace("janitor", quietly = TRUE)) {
  install.packages("janitor")
}
if (!requireNamespace("themis", quietly = TRUE)) {
  install.packages("themis")
}

library(caret)
library(themis)
library(skimr)
library(tidyverse)
library(janitor)
library(dplyr)

# Load the datasets
train <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")


# # Sample 10% of the train dataset
# set.seed(123) # Setting a seed for reproducibility
# train <- train %>% sample_frac(0.05)

# Display the first few rows of the train dataset
head(train)

# Get a summary of the dataset
glimpse(train)

# Skim the dataset for a comprehensive overview
skim(train)
```

```{r}
# Check for missing values
train_missing <- train %>% summarise_all(~sum(is.na(.)))
test_missing <- test %>% summarise_all(~sum(is.na(.)))
 
# Print missing values
print(train_missing)
print(test_missing)
```
We found that our dataset has no missing values at all. However to be future proofed, we prepared some handling for missing values below:
```{r}
# # Handle missing values
# # For numeric columns, fill missing values with median
# fill_missing_numeric <- function(df) {
#   df %>% mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))
# }
# 
# # For categorical columns, fill missing values with mode
# fill_missing_categorical <- function(df) {
#   df %>% mutate(across(where(is.character), ~ifelse(is.na(.), names(sort(table(.), decreasing = TRUE))[1], .)))
# }
# 
# # Apply the functions to train and test data
# train <- train %>% fill_missing_numeric() %>% fill_missing_categorical()
# test <- test %>% fill_missing_numeric() %>% fill_missing_categorical()
# 
# # Check for remaining missing values
# sum(is.na(train))
# sum(is.na(test))

```
##Data Cleaning 
```{r}
# Clean column names
train <- train %>% clean_names()
test <- test %>% clean_names()

# Handle duplicates
train <- train %>% distinct()
test <- test %>% distinct()

# # Correct inconsistencies --> do more if wanted?? -Dont think we need this for now. 
# # Standardize categorical levels
# train <- train %>% mutate(job_role = str_to_lower(job_role))
# test <- test %>% mutate(job_role = str_to_lower(job_role))

# Verify corrections
unique(train$job_role)
unique(test$job_role)

```
## Outlier detection and treatment  
Using IQR to detect and cap outliers  #TODO: Graphc and optimizing
```{r}
cap_outliers <- function(x) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = TRUE)
  caps <- quantile(x, probs=c(.05, .95), na.rm = TRUE)
  H <- 1.5 * IQR(x, na.rm = TRUE)
  lower_bound <- qnt[1] - H
  upper_bound <- qnt[2] + H
  x[x < lower_bound] <- caps[1]
  x[x > upper_bound] <- caps[2]
  return(x)
}

# Function to count outliers
count_outliers <- function(x) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = TRUE)
  H <- 1.5 * IQR(x, na.rm = TRUE)
  sum(x < (qnt[1] - H)) + sum(x > (qnt[2] + H))
}

# Apply the functions to train and test data
train_outlier_counts <- train %>% select(where(is.numeric)) %>% map_int(count_outliers)

# Cap outliers
train <- train %>% mutate_if(is.numeric, cap_outliers)

# Display the number of outliers detected and capped
print(train_outlier_counts)

```


```{r}
# Encode categorical variables using one-hot encoding
train <- train %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.integer))

test <- test %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.integer))

# Display structure after encoding
str(train)
str(test)

```

```{r}
# Normalize numerical features
num_cols <- train %>% select(where(is.numeric), -attrition) %>% colnames()
# num_cols <- train %>% select(where(is.numeric)) %>% colnames()


train[num_cols] <- scale(train[num_cols])
test[num_cols] <- scale(test[num_cols])

# Convert target variable to factor
train <- train %>%
  mutate(attrition = as.factor(attrition))
# test <- test %>%
#   mutate(attrition = as.factor(attrition))

# Display summary after normalization
summary(train[num_cols])
summary(test[num_cols])
```


```{r}
library(themis)
# Check for imbalanced data in the target variable
table(train$attrition)

# Handle imbalanced data using SMOTE from themis package
rec <- recipe(attrition ~ ., data = train) %>%
  step_smote(attrition)

train_balanced = train #TODO: REPLACE
# train_balanced <- prep(rec, training = train) %>% bake(new_data = NULL)

# Display summary of balanced data
table(train_balanced$attrition)

```

# Remove redundant features - check later if we want this
```{r}
# Using nearZeroVar function from caret package to identify near zero variance predictors
nzv <- nearZeroVar(train_balanced, saveMetrics = TRUE)
nzv <- rownames(nzv[nzv$nzv == TRUE,])

train_balanced <- train_balanced %>% select(-all_of(nzv))
test <- test %>% select(-all_of(nzv))

# Display structure after removing redundant features
str(train_balanced)
str(test)

```



```{r}
# Save the cleaned data for future use
write_csv(train_balanced, "data/processed_train.csv")
write_csv(test, "data/processed_test.csv")

```


Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

```{r}

# Convert target variable to factor with valid R variable names as levels
train_balanced$attrition <- factor(train_balanced$attrition, levels = c(1, 2), labels = c("No", "Yes"))
test$attrition <- factor(test$attrition, levels = c(1, 2), labels = c("No", "Yes"))

```



# Model Building and Comparison

```{r}
library(caret)
library(pROC)
library(doParallel)
library(gbm)
library(e1071)

# Convert target variable to factor
train_balanced$attrition <- as.factor(train_balanced$attrition)
test$attrition <- as.factor(test$attrition)

# # Register parallel backend
# registerDoParallel(detectCores())

# Define control for cross-validation
train_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  allowParallel = TRUE
)

# Train Logistic Regression model
set.seed(123)
model_logistic <- train(attrition ~ ., data = train_balanced,
                        method = "glm",
                        family = "binomial",
                        trControl = train_control,
                        metric = "ROC")

# Train kNN model
set.seed(123)
model_knn <- train(attrition ~ ., data = train_balanced,
                   method = "knn",
                   trControl = train_control,
                   tuneLength = 10,
                   metric = "ROC")

# Train Decision Tree model
set.seed(123)
model_tree <- train(attrition ~ ., data = train_balanced,
                    method = "rpart",
                    trControl = train_control,
                    metric = "ROC")

# Train Random Forest model
set.seed(123)
model_rf <- train(attrition ~ ., data = train_balanced,
                  method = "rf",
                  trControl = train_control,
                  metric = "ROC")

# Train Gradient Boosting model
set.seed(123)
model_gbm <- train(attrition ~ ., data = train_balanced,
                   method = "gbm",
                   trControl = train_control,
                   metric = "ROC",
                   verbose = FALSE)

# Train Naive Bayes model
set.seed(123)
model_nb <- train(attrition ~ ., data = train_balanced,
                  method = "nb",
                  trControl = train_control,
                  metric = "ROC")

# Evaluate each model on the test set and save the model
evaluate_model <- function(model, data) {
predictions <- predict(model, newdata = data)
prob_predictions <- predict(model, newdata = data, type = "prob")
roc_curve <- roc(data$attrition, prob_predictions[, "Yes"])
auc <- auc(roc_curve)
confusion <- confusionMatrix(predictions, data$attrition, positive = "Yes")

return(list(confusion = confusion, auc = auc, model = model))
}

# Store the evaluation results
results <- list(
logistic = evaluate_model(model_logistic, test),
knn = evaluate_model(model_knn, test),
tree = evaluate_model(model_tree, test),
rf = evaluate_model(model_rf, test),
gbm = evaluate_model(model_gbm, test),
nb = evaluate_model(model_nb, test)
)

# Print formatted AUC values for each model
auc_values <- sapply(results, function(res) res$auc)
cat("Model AUC Values:\n")
print(auc_values)

# Print key metrics from the confusion matrices for each model
cat("\nConfusion Matrix Metrics:\n")
sapply(names(results), function(model_name) {
res <- results[[model_name]]
cm <- res$confusion
tp <- cm$table[2, 2] # True Positives
tn <- cm$table[1, 1] # True Negatives
fp <- cm$table[1, 2] # False Positives
fn <- cm$table[2, 1] # False Negatives
sensitivity <- cm$byClass['Sensitivity']
specificity <- cm$byClass['Specificity']
accuracy <- cm$overall['Accuracy']

cat("\nModel:", model_name)
cat(sprintf("\n Accuracy: %.2f", accuracy))
cat(sprintf("\n Sensitivity (Recall): %.2f", sensitivity))
cat(sprintf("\n Specificity: %.2f", specificity))
cat("\n")
})

# Selecting and detailing the best model based on AUC
best_model_name <- names(which.max(auc_values))
best_model <- results[[best_model_name]]$model
cat("\nBest model based on AUC is:", best_model_name, "\n")

# Save the best model
saveRDS(best_model, paste0("best_model_", best_model_name, ".rds"))

# # --- Stop Parallel Processing ---
# stopCluster(cl)

```


# Summary of the best model

```{r}
print(summary(best_model))

```

### Enhanced Automatic Feature Selection and Model Training

```{r}


library(caret)
library(randomForest)
library(gbm)
library(FSelector)
library(ROCR)
library(doParallel)
library(pROC)

# # Setup parallel processing (for model training)
# num_cores <- detectCores() - 1
# cl <- makeCluster(num_cores)
# registerDoParallel(cl)

# Common setup
set.seed(123)
control_cv <- trainControl(method = "cv",
number = 10,
savePredictions = "final",
classProbs = TRUE,
allowParallel = TRUE)

# Function to find best features, train model, and plot feature importance
feature_select_train_evaluate_plot <- function(model_method, model_name, rfe_control_functions) {

feature_range <- seq(3, 19, by = 2)
best_feature_size <- NULL
max_auc <- 0

for (num_features in feature_range) {
cat("Evaluating", model_name, "with", num_features, "features\n")

control_rfe <- rfeControl(functions = rfe_control_functions,
method = "cv",
number = 10,
allowParallel = FALSE)

results_rfe <- rfe(x = train_balanced[, -ncol(train_balanced)],
y = train_balanced$attrition,
sizes = num_features,
rfeControl = control_rfe)

selected_features <- predictors(results_rfe)
train_subset <- train_balanced[, c(selected_features, "attrition")]

model_fit <- train(attrition ~ .,
data = train_subset,
method = model_method,
trControl = control_cv)

predictions <- model_fit$pred
roc_curve <- roc(predictions$obs, predictions$Yes)
auc_value <- auc(roc_curve)

cat("AUC for", num_features, "features:", auc_value, "\n\n")

if (auc_value > max_auc) {
max_auc <- auc_value
best_feature_size <- num_features
best_model <- model_fit
best_features <- selected_features
}
}

if (model_method != "nb"){
# --- Feature Importance Plot ---
importance <- varImp(best_model, scale = FALSE)$importance
importance <- importance[order(importance$Overall, decreasing = TRUE), , drop = FALSE]

par(mar = c(5, 10, 4, 2) + 0.1)
barplot(importance$Overall,
names.arg = rownames(importance),
horiz = TRUE,
las = 1,
main = paste("Feature Importance -", model_name),
xlab = "Importance",
col = "cornflowerblue")

}

cat("Best number of features for", model_name, ":", best_feature_size, "with AUC:", max_auc, "\n")

return(list(best_model = best_model, best_features = best_features, auc = max_auc, best_feature_size = best_feature_size))
}

# --- Model Training, Feature Selection, and Plotting ---

# Naive Bayes
naive_bayes_results <- feature_select_train_evaluate_plot("nb", "Naive Bayes", nbFuncs)

# Gradient Boosting
gradient_boosting_results <- feature_select_train_evaluate_plot("gbm", "Gradient Boosting", gamFuncs)

# Random Forest
random_forest_results <- feature_select_train_evaluate_plot("rf", "Random Forest", rfFuncs)

# # --- Stop Parallel Processing ---
# stopCluster(cl)

# Print Final Results
cat("\n\nFinal AUC Results Comparison:\n")
cat(paste("Naive Bayes AUC:", naive_bayes_results$auc, "with", naive_bayes_results$best_feature_size, "features\n"))
cat(paste("Gradient Boosting AUC:", gradient_boosting_results$auc, "with", gradient_boosting_results$best_feature_size, "features\n"))
cat(paste("Random Forest AUC:", random_forest_results$auc, "with", random_forest_results$best_feature_size, "features\n"))

```



```{r ROC Curve}
library(pROC)

# Get predicted probabilities for the best models on the test set
prob_nb <- predict(naive_bayes_results$best_model, newdata = test[, naive_bayes_results$best_features], type = "prob")[, "Yes"]
prob_gbm <- predict(gradient_boosting_results$best_model, newdata = test[, gradient_boosting_results$best_features], type = "prob")[, "Yes"]
prob_rf <- predict(random_forest_results$best_model, newdata = test[, random_forest_results$best_features], type = "prob")[, "Yes"]

# Calculate ROC curves
roc_nb <- roc(test$attrition, prob_nb)
roc_gbm <- roc(test$attrition, prob_gbm)
roc_rf <- roc(test$attrition, prob_rf)

# Plot ROC curves
plot(roc_nb, col = "blue", main = "ROC Curves for Top 3 Feature Selected Models", lty = 1)
plot(roc_gbm, add = TRUE, col = "red", lty = 2)
plot(roc_rf, add = TRUE, col = "green", lty = 3)

# Add legend
legend("bottomright", legend = c("Naive Bayes", "Gradient Boosting", "Random Forest"),
col = c("blue", "red", "green"), lty = 1:3)

```


```{r}
# Hyperparameter Optimization for Gradient Boosting Regressor

# Define the hyperparameter grid
gbm_grid <- expand.grid(
n.trees = c(50, 100, 200),
interaction.depth = c(2, 4, 6),
shrinkage = c(0.01, 0.1, 0.2),
n.minobsinnode = c(5, 10, 15)
)

# Create a trainControl object for repeated cross-validation
train_control_gbm <- trainControl(
method = "repeatedcv",
number = 5,
repeats = 3,
classProbs = TRUE,
summaryFunction = twoClassSummary,
allowParallel = TRUE
)

# Extract best features from previous Gradient Boosting run
best_gbm_features <- gradient_boosting_results$best_features
train_subset_gbm <- train_balanced[, c(best_gbm_features, "attrition")]

# Train the model with hyperparameter tuning
set.seed(123)
gbm_tuned <- train(attrition ~ .,
data = train_subset_gbm,
method = "gbm",
trControl = train_control_gbm,
tuneGrid = gbm_grid,
metric = "ROC",
verbose = FALSE)

# Print the best tuned model
print(gbm_tuned)

# Evaluate the tuned GBM model on the test set
gbm_tuned_pred <- predict(gbm_tuned, newdata = test[, best_gbm_features], type = "prob")
gbm_tuned_roc <- roc(test$attrition, gbm_tuned_pred[, "Yes"])
gbm_tuned_auc <- auc(gbm_tuned_roc)

cat("Tuned Gradient Boosting AUC:", gbm_tuned_auc, "\n")
```

```{r}
# Plot ROC Curve for the tuned GBM model
gbm_tuned_pred <- predict(gbm_tuned, newdata = test[, best_gbm_features], type = "prob")
gbm_tuned_roc <- roc(test$attrition, gbm_tuned_pred[, "Yes"])

plot(gbm_tuned_roc,
main = "ROC Curve for Tuned GBM Model",
col = "blue",
lwd = 2,
print.auc = TRUE,
auc.polygon = TRUE,
grid = TRUE)
```


```{r}
# Function to predict attrition for a new data point
predict_attrition <- function(model, features) {
# Create a data frame from the input features
new_data <- data.frame(t(features))

# Make the prediction using the trained model
prediction <- predict(model, new_data, type = "prob")[, "Yes"]

# Return the predicted probability of attrition
return(prediction)
}

# Example usage:
# Define the features for the new data point, MAKE SURE TO USE THE SAME ORDER AS IN YOUR TRAINING DATA
new_data_features <- c(
age = 0.5,
gender = 1,
years_at_company = 2,
job_role = 3,
monthly_income = 4,
work_life_balance = 3,
job_satisfaction = 2,
performance_rating = 4,
number_of_promotions = 1,
overtime = 1,
distance_from_home = 5,
education_level = 3,
marital_status = 2,
number_of_dependents = 1,
job_level = 2,
company_size = 3,
company_tenure = 6,
remote_work = 0,
innovation_opportunities = 4,
company_reputation = 4,
employee_recognition = 0.8
)

# Predict attrition using the tuned GBM model
predicted_attrition <- predict_attrition(gbm_tuned, new_data_features)

# Print the predicted probability
cat("Predicted probability of attrition:", predicted_attrition)
```





